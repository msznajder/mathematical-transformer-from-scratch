{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a91113-9b57-4cb0-ae5a-37b7642bafd0",
   "metadata": {
    "id": "10a91113-9b57-4cb0-ae5a-37b7642bafd0"
   },
   "source": [
    "## Intro\n",
    "\n",
    "Goal: Build LLM model from scratch - and train it to learn basic integer based math with addition, subtraction and multiplication.\n",
    "I want to check how well LLM - like this one just trained from scratch - can learn math. This is not a symbolic system. It uses tokens. Can tokens be used for math calculations? In further steps:\n",
    "1) Maybe embedding representing tokens are in fact kind of symbolic representation? This is worth checking - how these numbers and operations embedding behave and what are their relations.\n",
    "2) If vanilla LLMs with token embeddings will prove not very good at math can we somehow expand them to somehow get/arrive at/generate symbolic math operation within such model? I mean here creating LLM models with internal represenations with a flavour of Wolfram Alpha/Mathematica.\n",
    "\n",
    "I mean here the fact that LLMs are - and will be more and more - used for mathy problems. For example recently Microsoft started offering \"Copilot for Finance\" based on ChatGPT. I imaging that it is based on token and wonder how reliable the math side of it can be. And maybe: how could such models can be modified - both in terms of their architectures and the way they represent stuff like math - to be better at math. For now elementary like integer addition, subtraction and multiplication operations.\n",
    "\n",
    "Let's do it :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a55581-4c34-45c8-8b86-d85f6a410808",
   "metadata": {
    "id": "70a55581-4c34-45c8-8b86-d85f6a410808"
   },
   "source": [
    "## Plan\n",
    "1. ~~Generating and processing a small initial dataset to be able perfrom initial model training~~\n",
    "2. Format the dataset to right format, save as Dataset and push to HF Hub.\n",
    "3. Create a custom tokenizer for our dataset\n",
    "4. ~~Design and implement model architecture~~\n",
    "5. Design training goal (e.g. add some masking like in original transformers paper or no masking training)\n",
    "6. Training a model on GPU\n",
    "7. Gathering and processing a very large dataset (multiple operation beyond addition etc., complex structure of operations etc.)\n",
    "9. Reiterate other steps and refine the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c1513-b514-4dd3-b14a-ce0251c7187c",
   "metadata": {
    "id": "a11c1513-b514-4dd3-b14a-ce0251c7187c"
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Should I build Transformer encoder model or Transformer decoder model?\n",
    "\n",
    "* How to generate math data?\n",
    "\n",
    "\n",
    "* I should think whether math should be just strings of words or lists of tokens like in NER/POS?  \n",
    "* I think I want to build encoder based transformer model to account for deeper bidirectional relations catching\n",
    "* Or maybe keep it decoder model for better results generation?\n",
    "* I think encoder model is better here as I want model to be able to analyze all the relations at once and bidirectionally - not like generative model.\n",
    "\n",
    "\n",
    "* If I will implement the original \"Attention is all you need\" encoder part of the model (more or less) how many parameters will it have? Will it have billions? How many paramters has the paper model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2a279-edc0-4927-a850-62fa863f5514",
   "metadata": {
    "id": "14b2a279-edc0-4927-a850-62fa863f5514"
   },
   "source": [
    "## Notes to myself\n",
    "* Build Transformer-like model for generating math.\n",
    "* We will see how to work with huge amounts of data we want.\n",
    "* We will need to generate huge amounts of data.\n",
    "* I will explore the pretraining step and how to train a transformer model from scratch\n",
    "* We will have to cover following steps:\n",
    "    * Gathering and processing a very large dataset\n",
    "    * Creating a custom tokenizer for our dataset\n",
    "    * Training a model on GPU\n",
    "* To train such large mode we will probably use distributed trainig using some capabilities of PyTorch Accelerate library.\n",
    "* I will probably use notebook to prototype and experiment, but the final preprocessing and training code should probably be placed in script run potentially with multiple GPU's - but we'll see.\n",
    "* For example such script check out the Transformers repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdbc69-3e79-4395-82ee-2138e2c92177",
   "metadata": {
    "id": "1fcdbc69-3e79-4395-82ee-2138e2c92177"
   },
   "source": [
    "## 1. Generating and processing a small initial dataset\n",
    "\n",
    "* For now I can train initial model on some very small and structurally simple experimental dataset.\n",
    "* I think I will initially train model in addition from for numbers 0 to 99.\n",
    "* Model will also have to learn to understand what these numbers mean.\n",
    "* Starting with such a simple example I can ignore for now engineering the train dataset from something and focus on coding the model and training pipelines and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb59e35-cabb-45d6-bce2-d66971f82d86",
   "metadata": {
    "id": "efb59e35-cabb-45d6-bce2-d66971f82d86"
   },
   "source": [
    "Let's generate all four element combinations of four arrays containing numbers from 0 to 99.\n",
    "\n",
    "That's 100M initial addition data records to teach our model how to add numbers from 0 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfbde57-3440-412a-93b6-2482219e6e4d",
   "metadata": {
    "id": "4bfbde57-3440-412a-93b6-2482219e6e4d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nums = np.stack(np.meshgrid(np.arange(100), np.arange(100), np.arange(100), np.arange(100)), -1).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f51d10-a407-4912-8f92-754d0ca03cfc",
   "metadata": {
    "id": "69f51d10-a407-4912-8f92-754d0ca03cfc",
    "outputId": "89bf3c7d-3dc0-48d1-e002-9c85e06d5f00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  2],\n",
       "       ...,\n",
       "       [99, 99, 99, 97],\n",
       "       [99, 99, 99, 98],\n",
       "       [99, 99, 99, 99]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249dc93-11b9-40d9-b177-9d18d4daafde",
   "metadata": {
    "id": "2249dc93-11b9-40d9-b177-9d18d4daafde"
   },
   "source": [
    "We will now create DataFrame with these pairs and sum of each row. We will save this as csv file for our temporary model training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed2364-187f-46a6-ba6e-4a49eda923b1",
   "metadata": {
    "id": "77ed2364-187f-46a6-ba6e-4a49eda923b1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sums = pd.DataFrame({\"a\": nums[:, 0], \"b\": nums[:, 1], \"c\": nums[:, 2], \"d\": nums[:, 3], \"sum\": nums[:, 0]+nums[:, 1]+nums[:, 2]+nums[:, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c1f31-9f12-4ca8-b431-d1a77d3064e3",
   "metadata": {
    "id": "d63c1f31-9f12-4ca8-b431-d1a77d3064e3",
    "outputId": "5421e3f2-74d0-4abd-be0b-42d49293ee59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999995</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999996</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999997</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>97</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999998</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999999</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a   b   c   d  sum\n",
       "0          0   0   0   0    0\n",
       "1          0   0   0   1    1\n",
       "2          0   0   0   2    2\n",
       "3          0   0   0   3    3\n",
       "4          0   0   0   4    4\n",
       "...       ..  ..  ..  ..  ...\n",
       "99999995  99  99  99  95  392\n",
       "99999996  99  99  99  96  393\n",
       "99999997  99  99  99  97  394\n",
       "99999998  99  99  99  98  395\n",
       "99999999  99  99  99  99  396\n",
       "\n",
       "[100000000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964cc2a-842b-427f-a62a-59c5b36274d5",
   "metadata": {
    "id": "9964cc2a-842b-427f-a62a-59c5b36274d5"
   },
   "outputs": [],
   "source": [
    "sums.to_csv(\"./data/sums_4v_0_99.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af489f-bda9-44fb-93f0-e5b57d035421",
   "metadata": {
    "id": "e8af489f-bda9-44fb-93f0-e5b57d035421"
   },
   "source": [
    "That small initial dataset with sums of 4 numbers ranging from 0 to 99 has 1.5GB (400MB when compressed) of data only for this simple operation. We saved this dataset to csv file for later use in other places.\n",
    "\n",
    "Let's treat this as our base training dataset to teach our simplest model how to add numbers from 0 to 99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2ecf2-e96f-486c-9af1-832c31faf780",
   "metadata": {
    "id": "b9f2ecf2-e96f-486c-9af1-832c31faf780"
   },
   "source": [
    "## 2. Preprocess data and save to hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ccbd4-a1ff-42d5-a468-e0b5029d34ae",
   "metadata": {
    "id": "3a8ccbd4-a1ff-42d5-a468-e0b5029d34ae"
   },
   "outputs": [],
   "source": [
    "# We will implement it next - decided to first implement model architecture so we can fit the shape of dataset to the final architecture design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2339df7-e4d2-496c-b7af-b7d924dd712b",
   "metadata": {
    "id": "b2339df7-e4d2-496c-b7af-b7d924dd712b"
   },
   "source": [
    "## 3. Create a custom tokenizer for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1005ef0-500b-464f-9374-11e151538fef",
   "metadata": {
    "id": "a1005ef0-500b-464f-9374-11e151538fef"
   },
   "outputs": [],
   "source": [
    "# We will implement it next - decided to first implement model architecture so we can fit the tokenizer's nuts and bolts to the final architecture design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be75d7-fc9a-4a50-8f8b-3e33bd71e758",
   "metadata": {
    "id": "b2be75d7-fc9a-4a50-8f8b-3e33bd71e758"
   },
   "source": [
    "## 4. Design and implement model architecture\n",
    "\n",
    "Just because I want to build something simple - as much as transformers are concerned of course - I decided to use in this experiment the original transformers architecture presented in the \"Attention Is All You Need\" paper. To be more specific - the encoder part of it.\n",
    "\n",
    "We need only encoder because of two reasons. First, we do not solve a seq2seq problem like the original paper did. Second, I think that we should care more about building model that creates rich numerical representation of input data relations (e.g. tokens, mathematical relations, etc.) and that is what encoder mostly does best. Decoder is more kind of one directional with applied masking and puts more emphasis on the sequence generation - which is obvious thing when you think it is the second \"seq\" in the \"seq2seq\".\n",
    "\n",
    "(Sidenote: I now think it could be possible to pose this problem also as seq2seq: the mathematical expression being input sequence (e.g. \"2 + 2\") and the operation mathematical result being the output sequence (e.g. \"4\"). This would model the basic math operations as kind of translation problem - which in fact it kind of is. Just an idea for next implementation - we stick to encoder-only architecture for now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58374dcb-bf44-4dd9-be6a-fbb94eee7b66",
   "metadata": {
    "id": "58374dcb-bf44-4dd9-be6a-fbb94eee7b66"
   },
   "source": [
    "### 4.1 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vo9vzgO_9LwT",
   "metadata": {
    "id": "vo9vzgO_9LwT"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cLt6skfoAJJu",
   "metadata": {
    "id": "cLt6skfoAJJu"
   },
   "source": [
    "We first create the configuration for the architecture params. I mostly use values for the \"Attention Is All You Need\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "mCTbAjgk6TTG",
   "metadata": {
    "id": "mCTbAjgk6TTG"
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "class Config(PretrainedConfig):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.vocab_size = 100\n",
    "    self.embed_dim = 512\n",
    "    self.num_attention_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "WwPoVBwP6aan",
   "metadata": {
    "id": "WwPoVBwP6aan"
   },
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WUv0gFrEAYnR",
   "metadata": {
    "id": "WUv0gFrEAYnR"
   },
   "source": [
    "Just for prototyping I create some mockup tokenized sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "oZjADR6W81Ad",
   "metadata": {
    "id": "oZjADR6W81Ad"
   },
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n",
    "tokenized_inputs = BatchEncoding({\"input_ids\": torch.tensor([[1, 2, 3, 4]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9898567e-ee09-41e8-805a-6c9d46765857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9898567e-ee09-41e8-805a-6c9d46765857",
    "outputId": "d989dce8-94b4-4269-b3e0-c5e7f325e77d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "Ph3wpur3Ag__",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph3wpur3Ag__",
    "outputId": "e965c206-67d7-429b-e89f-854ed8348f06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs.input_ids.shape # [batch_size, tokenized_sequence_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_n-n7oxAymI",
   "metadata": {
    "id": "H_n-n7oxAymI"
   },
   "source": [
    "Also for prototyping we initialize embedding layer and use it to get embeddings for our example sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "YM6jheze6RwF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YM6jheze6RwF",
    "outputId": "0eaf144a-c204-4955-f94e-949555919391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 512)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "embeddings # [vocab_size, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28688029-2264-4635-a23f-9cbed16cd64e",
   "metadata": {
    "id": "28688029-2264-4635-a23f-9cbed16cd64e"
   },
   "outputs": [],
   "source": [
    "inputs_embeds = embeddings(tokenized_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37ad9d84-c85d-4435-b8fd-e9490f343862",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37ad9d84-c85d-4435-b8fd-e9490f343862",
    "outputId": "047a314c-77a1-43e6-af85-b3d675f72af0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds.size() # [batch_size, tokenized_sequence_length, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba8a7797-334b-4296-bb77-53318de85a08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba8a7797-334b-4296-bb77-53318de85a08",
    "outputId": "161bad1b-7da0-4649-9ba8-74b44bd58faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5223, -0.1004,  0.4019,  ..., -0.7209,  0.1582,  0.4808],\n",
       "         [-1.2000,  1.3288, -1.4894,  ..., -0.5783,  0.6096,  2.3303],\n",
       "         [ 0.4402,  1.4194, -1.3263,  ..., -0.1755,  0.1818, -0.0594],\n",
       "         [ 0.0694,  0.7195,  0.3353,  ...,  1.0410,  0.0963,  1.1397]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KF9iE9qvBJhR",
   "metadata": {
    "id": "KF9iE9qvBJhR"
   },
   "source": [
    "We now implement the key function used for estimating basically all the self-attention mechanism calculation as defined in the \"Attention Is All You Need\" paper.\n",
    "\n",
    "So we start with query `Q`, key `K` and value `V` as function input. Their value are basically hidden state representation weighted by set of weights trained by network separately for each of those three. We will implement it in a moment late and here we just take these value as the function arguments.\n",
    "\n",
    "We then set the `d_k` param value which is basically the hidden state representation dimensionality. We will use it for scaling the dot product as specified in original transformer paper. In the mockup data we use here it is 512.\n",
    "\n",
    "`QK_T` is a dot product between the input `Q` and `K` vectors measuring the similarity between each of them. We normalize this score by dividing it by the size of the vectors to avoid huge values for high-dimensional representions.\n",
    "\n",
    "`attention_weights` is the softmax of the `QK_T` similarity score to basically normalize the calculated weights to take values from 0 to 1 for each element and sum up to 1 for all of them.\n",
    "\n",
    "Finally, we calculate the `attention` value itself which is simply a dot product between calculated earlier attention weights for all sequence elements and the original values vector `V`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "223f4cb8-0afd-4ec1-b6bb-d433a5e68a3a",
   "metadata": {
    "id": "223f4cb8-0afd-4ec1-b6bb-d433a5e68a3a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention_Q_K_V(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    QK_T = torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(d_k)\n",
    "    attention_weights = torch.nn.functional.softmax(QK_T, dim=-1)\n",
    "    attention = torch.bmm(attention_weights, V)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I5jzij3ME5gT",
   "metadata": {
    "id": "I5jzij3ME5gT"
   },
   "source": [
    "We initialize the attention head with each of the query `Q`, key `K` and value `V` values as linear dense layer with dimentionality of size of embeddings and head dimensionality.\n",
    "\n",
    "Then on the forward step we input each of these three with the same current hidden state representation. Because these three `Q`, `K` and `V` layers are trained separately and they have their own sets of trained weights, the values of the query, key and value will be different from each other even though generated from the same hidden state.\n",
    "\n",
    "We then feed these values into the attention value calculation function and return attention value on each `Attention` forward step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a825c628-c68d-472f-a668-f4ba3cbdd442",
   "metadata": {
    "id": "a825c628-c68d-472f-a668-f4ba3cbdd442"
   },
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.Q = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.K = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.V = torch.nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attention = attention_Q_K_V(self.Q(hidden_state), self.K(hidden_state), self.V(hidden_state))\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QBbfX8f5H5yY",
   "metadata": {
    "id": "QBbfX8f5H5yY"
   },
   "outputs": [],
   "source": [
    "# NEXT: Implement MultiHeadAttention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7182a43-05d9-4d00-8b86-3ec345f0348a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ef3dc-6c3a-4dcf-b2c7-c0f19d67de60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8bd05-6447-4afa-a6a3-5740278e1f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jqrB34Gx66fC",
   "metadata": {
    "id": "jqrB34Gx66fC"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hQUH8O5n66jY",
   "metadata": {
    "id": "hQUH8O5n66jY"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement residual layers connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9MVd7k07KJa",
   "metadata": {
    "id": "l9MVd7k07KJa"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iMR2gix07P-r",
   "metadata": {
    "id": "iMR2gix07P-r"
   },
   "outputs": [],
   "source": [
    "# TODO: Put it all together and add classification (?) head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfeeb09-f58c-4921-8e3b-d2e03ef13eeb",
   "metadata": {
    "id": "6cfeeb09-f58c-4921-8e3b-d2e03ef13eeb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fcdd3-1dc6-40a9-90ef-661a682a70a7",
   "metadata": {
    "id": "ee2fcdd3-1dc6-40a9-90ef-661a682a70a7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
