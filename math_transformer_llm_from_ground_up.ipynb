{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a91113-9b57-4cb0-ae5a-37b7642bafd0",
   "metadata": {},
   "source": [
    "## Intro - remove \"we\" and an \"I\"\n",
    "\n",
    "Goal: Build LLM model from scratch - not pre-trained - and train it to learn basic integer based math with addition, subtraction and multiplication. \n",
    "I want to check how well LLM - like this one just trained from scratch - can learn math. This is not a symbolic system. It uses tokens. Can tokens be used for math calculations? In further steps: \n",
    "1) Maybe embedding representing tokens are in fact kind of symbolic representation? This is worth checking - how these numbers and operations embedding behave and what are their relations.\n",
    "2) If vanilla LLMs with token embeddings will prove not very good at math can we somehow expand them to somehow get/arrive at/generate symbolic math operation within such model? I mean here creating LLM models with internal represenations with a flavour of Wolfram Alpha/Mathematica.\n",
    "\n",
    "I mean here the fact that LLMs are - and will be more and more - used for mathy problems. For example recently Microsoft started offering \"Copilot for Finance\" based on ChatGPT. I imaging that it is based on token and wonder how reliable the math side of it can be. And maybe: how could such models can be modified - both in terms of their architectures and the way they represent stuff like math - to be better at math. For now elementary like integer addition, subtraction and multiplication operations.\n",
    "\n",
    "Let's do it :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a55581-4c34-45c8-8b86-d85f6a410808",
   "metadata": {},
   "source": [
    "## Plan\n",
    "1. ~~Generating and processing a small initial dataset to be able perfrom initial model training~~\n",
    "2. Format the dataset to right format, save as Dataset and push to HF Hub.\n",
    "3. Create a custom tokenizer for our dataset\n",
    "4. ~~Design and implement model architecture~~\n",
    "5. Design training goal (e.g. add some masking like in original transformers paper or no masking training)\n",
    "6. Training a model on GPU\n",
    "7. Gathering and processing a very large dataset (multiple operation beyond addition etc., complex structure of operations etc.)\n",
    "9. Reiterate other steps and refine the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c1513-b514-4dd3-b14a-ce0251c7187c",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "* Should I build Transformer encoder model or Transformer decoder model?\n",
    "\n",
    "* How to generate math data?\n",
    "\n",
    "\n",
    "* I should think whether math should be just strings of words or lists of tokens like in NER/POS?  \n",
    "* I think I want to build encoder based transformer model to account for deeper bidirectional relations catching\n",
    "* Or maybe keep it decoder model for better results generation?\n",
    "* I think encoder model is better here as I want model to be able to analyze all the relations at once and bidirectionally - not like generative model.\n",
    "\n",
    "\n",
    "* If I will implement the original \"Attention is all you need\" encoder part of the model (more or less) how many parameter will it have? Will it have billions? How many paramters has the paper model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2a279-edc0-4927-a850-62fa863f5514",
   "metadata": {},
   "source": [
    "## Notes to myself\n",
    "* Build Transformer-like model for generating math.\n",
    "* We will see how to work with huge amounts of data we want.\n",
    "* We will need to generate huge amounts of data.\n",
    "* I will explore the pretraining step and how to train a transformer model from scratch\n",
    "* We will have to cover following steps:\n",
    "    * Gathering and processing a very large dataset\n",
    "    * Creating a custom tokenizer for our dataset\n",
    "    * Training a model on GPU\n",
    "* To train such large mode we will probably use distributed trainig using some capabilities of PyTorch Accelerate library.\n",
    "* I will probably use notebook to prototype and experiment, but the final preprocessing and training code should probably be placed in script run potentially with multiple GPU's - but we'll see.\n",
    "* For example such script check out the Transformers repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdbc69-3e79-4395-82ee-2138e2c92177",
   "metadata": {},
   "source": [
    "## 1. Generating and processing a small initial dataset\n",
    "\n",
    "* For now I can train initial model on some very small and structurally simple experimental dataset.\n",
    "* I think I will initially train model in addition from for numbers 0 to 99.\n",
    "* Model will also have to learn to understand what these numbers mean.\n",
    "* Starting with such a simple example I can ignore for now engineering the train dataset from something and focus on coding the model and training pipelines and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb59e35-cabb-45d6-bce2-d66971f82d86",
   "metadata": {},
   "source": [
    "Let's generate all four element combinations of four arrays containing numbers from 0 to 99. \n",
    "\n",
    "That's 100M initial addition data records to teach our model how to add numbers from 0 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfbde57-3440-412a-93b6-2482219e6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nums = np.stack(np.meshgrid(np.arange(100), np.arange(100), np.arange(100), np.arange(100)), -1).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f51d10-a407-4912-8f92-754d0ca03cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  2],\n",
       "       ...,\n",
       "       [99, 99, 99, 97],\n",
       "       [99, 99, 99, 98],\n",
       "       [99, 99, 99, 99]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249dc93-11b9-40d9-b177-9d18d4daafde",
   "metadata": {},
   "source": [
    "We will now create DataFrame with these pairs and sum of each row. We will save this as csv file for our temporary model training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ed2364-187f-46a6-ba6e-4a49eda923b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sums = pd.DataFrame({\"a\": nums[:, 0], \"b\": nums[:, 1], \"c\": nums[:, 2], \"d\": nums[:, 3], \"sum\": nums[:, 0]+nums[:, 1]+nums[:, 2]+nums[:, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63c1f31-9f12-4ca8-b431-d1a77d3064e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999995</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999996</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999997</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>97</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999998</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999999</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a   b   c   d  sum\n",
       "0          0   0   0   0    0\n",
       "1          0   0   0   1    1\n",
       "2          0   0   0   2    2\n",
       "3          0   0   0   3    3\n",
       "4          0   0   0   4    4\n",
       "...       ..  ..  ..  ..  ...\n",
       "99999995  99  99  99  95  392\n",
       "99999996  99  99  99  96  393\n",
       "99999997  99  99  99  97  394\n",
       "99999998  99  99  99  98  395\n",
       "99999999  99  99  99  99  396\n",
       "\n",
       "[100000000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9964cc2a-842b-427f-a62a-59c5b36274d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums.to_csv(\"./data/sums_4v_0_99.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af489f-bda9-44fb-93f0-e5b57d035421",
   "metadata": {},
   "source": [
    "That small initial dataset with sums of 4 numbers ranging from 0 to 99 has 1.5GB (400MB when compressed) of data only for this simple operation. We saved this dataset to csv file for later use in other places.\n",
    "\n",
    "Let's treat this as our base training dataset to teach our simplest model how to add numbers from 0 to 99. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2ecf2-e96f-486c-9af1-832c31faf780",
   "metadata": {},
   "source": [
    "## 2. Preprocess data and save to hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ccbd4-a1ff-42d5-a468-e0b5029d34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement it next - decided to first implement model architecture so we can fit the shape of dataset to the final architecture design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2339df7-e4d2-496c-b7af-b7d924dd712b",
   "metadata": {},
   "source": [
    "## 3. Create a custom tokenizer for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1005ef0-500b-464f-9374-11e151538fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement it next - decided to first implement model architecture so we can fit the tokenizer's nuts and bolts to the final architecture design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be75d7-fc9a-4a50-8f8b-3e33bd71e758",
   "metadata": {},
   "source": [
    "## 4. Design and implement model architecture\n",
    "\n",
    "Just because I want to build something simple - as much as transformers are concerned of course - I decided to use in this experiment the original transformers architecture presented in the \"Attention Is All You Need\" paper. To be more specific - the encoder part of it. \n",
    "\n",
    "We need only encoder because of two reasons. First, we do not solve a seq2seq problem like the original paper did. Second, I think that we should care more about building model that creates rich numerical representation of input data relations (e.g. tokens, mathematical relations, etc.) and that is what encoder mostly does best. Decoder is more kind of one directional with applied masking and puts more emphasis on the sequence generation - which is obvious thing when you think it is the second \"seq\" in the \"seq2seq\".\n",
    "\n",
    "(Sidenote: I now think it could be possible to pose this problem also as seq2seq: the mathematical expression being input sequence (e.g. \"2 + 2\") and the operation mathematical result being the output sequence (e.g. \"4\"). This would model the basic math operations as kind of translation problem - which in fact it kind of is. Just an idea for next implementation - we stick to encoder-only architecture for now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58374dcb-bf44-4dd9-be6a-fbb94eee7b66",
   "metadata": {},
   "source": [
    "### 4.1 Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516bdd2-eba7-479c-af5d-8b31fe4b8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use BERT as initial example\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "CHECKPOINT = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedd42ef-87be-43d1-af9c-fba90ad1d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is example text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151fdd12-a032-4307-9182-73e5344a668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the text\n",
    "inputs = tokenier(text, return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b66756-4700-454a-ab61-babbd0b44146",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d140793e-428d-4cdb-8b6c-7eebf1481f17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ckpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[0;32m----> 6\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_ckpt\u001b[49m)\n\u001b[1;32m      7\u001b[0m token_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m      8\u001b[0m token_emb\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ckpt' is not defined"
     ]
    }
   ],
   "source": [
    "# We now build dense embeddings.\n",
    "# We will use model parameter for it.\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# TODO: Remove using BERT and config it ourselves.\n",
    "# We load bert-base-uncased config.json - to get the config params.\n",
    "# Here each input ID is mapped to one of 30522 embedding vectors stored in nn.Embedding\n",
    "# Each embedding has 768 dimensions\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28688029-2264-4635-a23f-9cbed16cd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created a lookup table to trainslate each input_ids into corresponding embedding\n",
    "# We do it by feeding the specific input id to the embedding lookup.\n",
    "input_embeds = token_emb(inputs.input_ids)\n",
    "input_embeds.size() # [batch_size, seq_len, hidden_dim] -> [1, 5, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c746c3b5-2386-4cb8-a871-a96424e39fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# We should now implement and add the positional embeddings component.\n",
    "# We skip it for now for simplicity and come back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a544e34-c88f-4991-9661-d1b01dc00f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now focus on implementing the query, key and values representations.\n",
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "# TODO:\n",
    "# We use here a vast simplification where Q, K and V are equal.\n",
    "# In fact they are mapped by dedicated trained set of weights.\n",
    "# We will also implement these later.\n",
    "\n",
    "query = key = value = input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6488ead-f928-4fda-95d7-4c03fbe9349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now calculate the attention scores dot products which are the embeddings similarity measure in the transformers architecture\n",
    "# Division by the embedding dimensionality length is a normalization step preventing problems with exploding gradients \n",
    "#  as the value without normalization in such high dimensionality can get huge.\n",
    "dim_k = key.size(-1)\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991ab6d-0896-4118-a527-ea6e4b62db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ad157-0238-4ee3-8b49-ac7c572012a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00721f-2a6e-402b-a334-b62b350c1548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167e64b-4a58-4e6f-aca0-cc553c00a20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f4cb8-0afd-4ec1-b6bb-d433a5e68a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825c628-c68d-472f-a668-f4ba3cbdd442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379af8d1-92c0-4a6a-8e4a-5cf9bd459697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37da4f-a41b-4258-8256-b4e8ac755de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9867f65-3bc6-4a3d-adb4-384838e2c6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23832e32-f99c-4f9a-91f3-28c3cbc11f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a899-ffb3-4a00-be39-dd6514973c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fcdd3-1dc6-40a9-90ef-661a682a70a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
