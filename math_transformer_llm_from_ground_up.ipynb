{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a91113-9b57-4cb0-ae5a-37b7642bafd0",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Goal: Build LLM model from scratch - not pre-trained - and train it to learn basic integer based math with addition, subtraction and multiplication. \n",
    "I want to check how well LLM - like this one just trained from scratch - can learn math. This is not a symbolic system. It uses tokens. Can tokens be used for math calculations? In further steps: \n",
    "1) Maybe embedding representing tokens are in fact kind of symbolic representation? This is worth checking - how these numbers and operations embedding behave and what are their relations.\n",
    "2) If vanilla LLMs with token embeddings will prove not very good at math can we somehow expand them to somehow get/arrive at/generate symbolic math operation within such model? I mean here creating LLM models with internal represenations with a flavour of Wolfram Alpha/Mathematica.\n",
    "\n",
    "I mean here the fact that LLM are - and will be more and more - used for mathy problems. For example recently Microsoft started offering \"Copilot for Finance\" based on ChatGPT. I imaging that it is based on token and wonder how reliable the math side of it can be. And maybe: how could such models can be modified - both in terms of their architectures and the way they represent stuff like math - to be better at math. For now elementary like integer addition, subtraction and multiplication operations.\n",
    "\n",
    "Let's do it :)\n",
    "\n",
    "\n",
    "* Build Transformer-like model for generating math.\n",
    "* We will see how to work with huge amounts of data we want.\n",
    "* We will need to generate huge amounts of data.\n",
    "* I will explore the pretraining step and how to train a transformer model from scratch\n",
    "* We will have to cover following steps:\n",
    "    * Gathering and processing a very large dataset\n",
    "    * Creating a custom tokenizer for our dataset\n",
    "    * Training a model on GPU\n",
    "* To train such large mode we will probably use distributed trainig using some capabilities of PyTorch Accelerate library.\n",
    "* I will probably use notebook to prototype and experiment, but the final preprocessing and training code should probably be placed in script run potentially with multiple GPU's - but we'll see.\n",
    "* For example such script check out the Transformers repository\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a55581-4c34-45c8-8b86-d85f6a410808",
   "metadata": {},
   "source": [
    "## General plan\n",
    "\n",
    "* Generating and processing a small initial dataset to be able perfrom initial model training\n",
    "* Format the dataset to right format, save as Dataset and push to HF Hub.\n",
    "* Create a custom tokenizer for our dataset\n",
    "* Design and implement the trained model architecture\n",
    "* Training a model on GPU\n",
    "\n",
    "* Gathering and processing a very large dataset\n",
    "    * multiple operation beyond addition etc.\n",
    "    * complex structure of operations etc.\n",
    "* Reiterate other steps and refine the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c1513-b514-4dd3-b14a-ce0251c7187c",
   "metadata": {},
   "source": [
    "## Problems to solve (messy notebook):\n",
    "\n",
    "* Should I build Transformer encoder model or Transformer decoder model?\n",
    "\n",
    "* How to generate math data?\n",
    "\n",
    "\n",
    "* I should think whether math should be just strings of words or lists of tokens like in NER/POS?  \n",
    "* I think I want to build encoder based transformer model to account for deeper bidirectional relations catching\n",
    "* Or maybe keep it decoder model for better results generation?\n",
    "* I think encoder model is better here as I want model to be able to analyze all the relations at once and bidirectionally - not like generative model.\n",
    "\n",
    "\n",
    "* If I will implement the original \"Attention is all you need\" encoder part of the model (more or less) how many parameter will it have? Will it have billions? How many paramters has the paper model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdbc69-3e79-4395-82ee-2138e2c92177",
   "metadata": {},
   "source": [
    "## 1. Building up large training dataset\n",
    "\n",
    "### 1.1 Xsw\n",
    "\n",
    "* For now I can train on some very small experimental - 100 samples dataset.\n",
    "* I think I will initially train model in addition from for numbers 1 to 100.\n",
    "* Model will also have to learn to understand what these numbers mean.\n",
    "* Starting with such a simple example I can ignore for now engineering the train dataset from something and focus on coding the model and training pipelines and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb59e35-cabb-45d6-bce2-d66971f82d86",
   "metadata": {},
   "source": [
    "Let's generate all four element combinations of four arrays containing numbers from 0 to 99. \n",
    "\n",
    "That's 100M initial addition data records to teach our model how to add numbers from 0 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfbde57-3440-412a-93b6-2482219e6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nums = np.stack(np.meshgrid(np.arange(100), np.arange(100), np.arange(100), np.arange(100)), -1).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f51d10-a407-4912-8f92-754d0ca03cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  2],\n",
       "       ...,\n",
       "       [99, 99, 99, 97],\n",
       "       [99, 99, 99, 98],\n",
       "       [99, 99, 99, 99]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249dc93-11b9-40d9-b177-9d18d4daafde",
   "metadata": {},
   "source": [
    "We will now create DataFrame with these pairs and sum of each row. We will save this as csv file for our temporary model training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ed2364-187f-46a6-ba6e-4a49eda923b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sums = pd.DataFrame({\"a\": nums[:, 0], \"b\": nums[:, 1], \"c\": nums[:, 2], \"d\": nums[:, 3], \"sum\": nums[:, 0]+nums[:, 1]+nums[:, 2]+nums[:, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63c1f31-9f12-4ca8-b431-d1a77d3064e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999995</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999996</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999997</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>97</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999998</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999999</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a   b   c   d  sum\n",
       "0          0   0   0   0    0\n",
       "1          0   0   0   1    1\n",
       "2          0   0   0   2    2\n",
       "3          0   0   0   3    3\n",
       "4          0   0   0   4    4\n",
       "...       ..  ..  ..  ..  ...\n",
       "99999995  99  99  99  95  392\n",
       "99999996  99  99  99  96  393\n",
       "99999997  99  99  99  97  394\n",
       "99999998  99  99  99  98  395\n",
       "99999999  99  99  99  99  396\n",
       "\n",
       "[100000000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9964cc2a-842b-427f-a62a-59c5b36274d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums.to_csv(\"./data/sums_4v_0_99.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af489f-bda9-44fb-93f0-e5b57d035421",
   "metadata": {},
   "source": [
    "That small initial dataset with sums of 4 numbers ranging from 0 to 99 has 1.5GB (... when compressed) of data only for this simple operation. We saved this dataset to csv file for later use in other places.\n",
    "\n",
    "Let's treat this as our base training dataset to teach our simplest model how to add numbers from 0 to 99. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85e2c0-cb99-4a0c-83a3-0ad316ffcc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee5fed-4a07-4780-a719-a53613e53f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5712e0a8-5235-4193-981a-edf22e35a8f4",
   "metadata": {},
   "source": [
    "NEXT: Build Hugging Face Dataset out of it. headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ccbd4-a1ff-42d5-a468-e0b5029d34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2339df7-e4d2-496c-b7af-b7d924dd712b",
   "metadata": {},
   "source": [
    "NEXT: Prepare Tokenizer Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1005ef0-500b-464f-9374-11e151538fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a6179-9775-43b2-85f6-92eda4d6c5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516bdd2-eba7-479c-af5d-8b31fe4b8866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3b667b-0b46-4b6b-930a-b16d09741d10",
   "metadata": {},
   "source": [
    "NEXT: Prepare Transformer Encoder basic architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7438fa-fc71-48c8-9689-4bb3d5a62ecc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd42ef-87be-43d1-af9c-fba90ad1d588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fdd12-a032-4307-9182-73e5344a668d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b66756-4700-454a-ab61-babbd0b44146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140793e-428d-4cdb-8b6c-7eebf1481f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28688029-2264-4635-a23f-9cbed16cd64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746c3b5-2386-4cb8-a871-a96424e39fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a544e34-c88f-4991-9661-d1b01dc00f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6488ead-f928-4fda-95d7-4c03fbe9349c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991ab6d-0896-4118-a527-ea6e4b62db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ad157-0238-4ee3-8b49-ac7c572012a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00721f-2a6e-402b-a334-b62b350c1548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167e64b-4a58-4e6f-aca0-cc553c00a20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f4cb8-0afd-4ec1-b6bb-d433a5e68a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825c628-c68d-472f-a668-f4ba3cbdd442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379af8d1-92c0-4a6a-8e4a-5cf9bd459697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37da4f-a41b-4258-8256-b4e8ac755de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9867f65-3bc6-4a3d-adb4-384838e2c6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23832e32-f99c-4f9a-91f3-28c3cbc11f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a899-ffb3-4a00-be39-dd6514973c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fcdd3-1dc6-40a9-90ef-661a682a70a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
